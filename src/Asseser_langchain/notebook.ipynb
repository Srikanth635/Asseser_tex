{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:21.188850Z",
     "start_time": "2025-01-22T12:38:21.179200Z"
    }
   },
   "source": [
    "from typing import Dict, Optional\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForChainRun\n",
    "\n",
    "from chains.chaining import *\n",
    "from prompt.prompt_template import *\n",
    "from llms.gpt import *\n",
    "from langchain_script import *\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm_client = OpenAI(model_name=\"gpt-3.5-turbo\",openai_api_key=OPENAI_API_KEY, temperature=0.7)"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:21.741372Z",
     "start_time": "2025-01-22T12:38:21.663456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "content_gen = extract_text_from_pdf_page_by_page(os.path.join(docs_dir, \"report.pdf\"))\n",
    "content_chunks = split_large_text(content_gen)\n",
    "prompt1_txt = document_reader(os.path.join(prompts_dir,'prompt1.txt'))\n",
    "prompt2_txt = document_reader(os.path.join(prompts_dir,'prompt2.txt'))\n",
    "prompt3_txt = document_reader(os.path.join(prompts_dir,'prompt3.txt'))\n",
    "prompt4_txt = document_reader(os.path.join(prompts_dir,'prompt4.txt'))"
   ],
   "id": "b8644b7574f762fc",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:22.067917Z",
     "start_time": "2025-01-22T12:38:22.065992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# writing_style = llm_chaining(llm=llm_client,prompt=initial_prompt_template,output_key=\"writing\")\n",
    "# research_quality = llm_chaining(llm=llm_client,prompt=chain_prompt_template,output_key=\"quality\")\n",
    "# flaw_report = llm_chaining(llm=llm_client,prompt=chain_prompt_template,output_key=\"flaws\")\n",
    "# suggestions = llm_chaining(llm=llm_client,prompt=chain_prompt_template,output_key=\"suggestions\")"
   ],
   "id": "eefcb3ebd17a2a14",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:22.543707Z",
     "start_time": "2025-01-22T12:38:22.541402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "writing_style_prompt = PromptTemplate(\n",
    "    input_variables=[\"prompt1\",\"document\"],\n",
    "    template=\"Analyze document based on objectives given as \\n\\n {prompt1} \\n\\n Document: \\n\\n {document} \\n\\n Analysis:\"\n",
    ")\n",
    "writing_style_chain = LLMChain(llm=llm, prompt=writing_style_prompt, output_key=\"writing\")"
   ],
   "id": "f4b7c626b62ce534",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:23.387813Z",
     "start_time": "2025-01-22T12:38:23.385765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "research_quality_prompt = PromptTemplate(\n",
    "    input_variables=[\"prompt2\",\"document\",\"writing\"],\n",
    "    template=\"Analyze document based on objectives given as \\n\\n {prompt1} \\n\\n Document: \\n\\n {document} \\n\\n also consider previous output\\n\\n Previous Output:\\n\\n {writing}\"\n",
    ")\n",
    "research_quality_chain = LLMChain(llm=llm, prompt=research_quality_prompt, output_key=\"quality\")"
   ],
   "id": "ef22c9b1b0141f50",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:25.395650Z",
     "start_time": "2025-01-22T12:38:25.393478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "flaw_report_prompt = PromptTemplate(\n",
    "    input_variables=[\"prompt3\",\"document\",\"quality\"],\n",
    "    template=\"Analyze document based on objectives given as \\n\\n {prompt1} \\n\\n Document: \\n\\n {document} \\n\\n also consider previous output\\n\\n Previous Output:\\n\\n {quality}\"\n",
    ")\n",
    "flaw_report_chain = LLMChain(llm=llm, prompt=flaw_report_prompt, output_key=\"flaws\")"
   ],
   "id": "d13ad96b50a642b6",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:25.683514Z",
     "start_time": "2025-01-22T12:38:25.681172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "suggestions_prompt = PromptTemplate(\n",
    "    input_variables=[\"prompt4\",\"document\",\"flaws\"],\n",
    "    template=\"Analyze document based on objectives given as \\n\\n {prompt1} \\n\\n Document: \\n\\n {document} \\n\\n also consider previous output\\n\\n Previous Output:\\n\\n {flaws}\"\n",
    ")\n",
    "suggestions_chain = LLMChain(llm=llm, prompt=suggestions_prompt, output_key=\"suggestions\")"
   ],
   "id": "ae00aea4948f3565",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:25.971737Z",
     "start_time": "2025-01-22T12:38:25.969726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seq_chain = SequentialChain(\n",
    "    chains=[writing_style_chain, research_quality_chain, flaw_report_chain, suggestions_chain],\n",
    "    input_variables=[\"document\",\"prompt1\",\"prompt2\",\"prompt3\",\"prompt4\"],\n",
    "    output_variables=[\"writing\",\"quality\",\"flaws\",\"suggestions\"]\n",
    ")"
   ],
   "id": "67c382a24a83d4f4",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:38:27.462535Z",
     "start_time": "2025-01-22T12:38:27.460673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_data = {\n",
    "    \"document\" : content_chunks[0],\n",
    "    \"prompt1\" : prompt1_txt,\n",
    "    \"prompt2\" : prompt2_txt,\n",
    "    \"prompt3\" : prompt3_txt,\n",
    "    \"prompt4\" : prompt4_txt,\n",
    "}"
   ],
   "id": "fdf3c17fa035ff42",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:39:01.808212Z",
     "start_time": "2025-01-22T12:38:27.924369Z"
    }
   },
   "cell_type": "code",
   "source": "result = seq_chain.invoke(input_data)",
   "id": "f47d2259ab3029e0",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:39:04.019482Z",
     "start_time": "2025-01-22T12:39:04.017428Z"
    }
   },
   "cell_type": "code",
   "source": "result.keys()",
   "id": "d214bbf590b768af",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['document', 'prompt1', 'prompt2', 'prompt3', 'prompt4', 'writing', 'quality', 'flaws', 'suggestions'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:39:11.518698Z",
     "start_time": "2025-01-22T12:39:11.516875Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('flaws'))",
   "id": "30cecf5f6bf1fb8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and colloquialisms could be removed for a more professional tone.\n",
      "\n",
      "**Recommendations:**\n",
      "- Ensure consistent definition and explanation of specialized terms and acronyms throughout the proposal.\n",
      "- Review and revise transitions between sections to improve overall coherence and flow.\n",
      "- Remove any informal phrases and colloquialisms for a more formal tone.\n",
      "\n",
      "\\section{Assessment of Writing Style}\n",
      "\n",
      "\\subsection{Clarity: 8/10}\n",
      "- Technical concepts and methodologies are explained clearly and concisely.\n",
      "- Language is accessible to an interdisciplinary audience while maintaining technical rigor.\n",
      "- Acronyms and specialized terms are consistently defined and explained, but there are a few inconsistencies.\n",
      "\n",
      "\\subsection{Precision: 9/10}\n",
      "- Writing avoids vague or ambiguous statements.\n",
      "- Objectives, methodologies, and outcomes are described with sufficient detail and specificity.\n",
      "- Claims are supported with appropriate data, evidence, or references.\n",
      "\n",
      "\\subsection{Coherence and Flow: 7/10}\n",
      "- Document follows a logical sequence of ideas.\n",
      "- Transitions between sections and subsections are mostly smooth and intuitive, but there are a few instances where they could be improved.\n",
      "- Repetitive or disjointed elements are minimized, but some sections may not flow smoothly.\n",
      "\n",
      "\\subsection{Professional Tone: 8/10\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:39:32.920132Z",
     "start_time": "2025-01-22T12:39:32.918360Z"
    }
   },
   "cell_type": "code",
   "source": "print(result.get('suggestions'))",
   "id": "c6547ffb10d63589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "}\n",
      "- Writing is formal and consistent with academic and scientific standards.\n",
      "- Tone reflects the ambition and significance of the research without overstating claims.\n",
      "- Some informal phrases and colloquialisms could be removed for a more professional tone.\n",
      "\n",
      "\\subsection{Engagement and Persuasiveness: 9/10}\n",
      "- Proposal captures the reader's attention with compelling arguments.\n",
      "- Societal, scientific, and economic impacts of the research are effectively communicated.\n",
      "- Unique aspects of the research are emphasized to distinguish it from similar initiatives.\n",
      "\n",
      "\\subsection{Grammar, Syntax, and Formatting: 8/10}\n",
      "- Sentences are well-constructed, grammatically correct, and free of typographical errors.\n",
      "- Formatting enhances readability, with consistent headings, bullet points, and lists.\n",
      "- Visuals, tables, and charts are integrated effectively into the narrative, but there may be room for improvement in their placement and use.\n",
      "\n",
      "\\subsection{Summary of Ratings and Findings}\n",
      "\n",
      "\\begin{tabular}{ |p{3cm}||p{2cm}|p{7cm}| }\n",
      " \\hline\n",
      " \\multicolumn{3}{|c|}{\\textbf{Writing Style Assessment}} \\\\\n",
      " \\hline\n",
      " \\textbf{Criterion} & \\textbf{Rating}\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T12:40:41.927998Z",
     "start_time": "2025-01-22T12:40:38.582350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_data_writing = {\n",
    "    \"document\" : content_chunks[0],\n",
    "    \"prompt1\" : prompt1_txt}\n",
    "print(writing_style_chain.invoke(input_data_writing))"
   ],
   "id": "ec5b52e983d80cf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 'Chapter2. ProbabilisticKnowledgeRepresentation\\nP(X|Y)=P(Y|X)P(X)\\nP(Y)(2.6)\\nBayes’ theorem allows to convert an abductive reasoning task into a deductive one\\nand vice versa. It plays a fundamental role in Bayesian networks, which model\\nuncertain causal relationships among random variables. Applying Bayes’ theorem\\nallows to turn a causal model into a diagnostic reasoning task.\\nInference It\\nPosterior\\nInferenceis obviously straightforward to convert a propositional logic KB into a\\nprobabilistic KB by introducing a Boolean random variable for every atomic proposi-\\ntion inΣ. The joint probability distribution P(Σ)then can be stored in a contingency\\ntable holding the co-occurrences of the respective atomic events. Making use of\\nthe chain rule, the law of total probability and Bayes’ theorem, one can computethe posterior probability\\nP(Q|E)of any arbitrary query Q⊆Σgiven any arbitrary\\nevidence E⊆Σ. Such a posterior belief can be computed by the canonical inference\\nequation (cf. Jain, 2012)', 'prompt1': '**Objective**: Assess the writing style of the Collaborative Research Centre (CRC) proposal for the EASE project.\\nEvaluate the clarity, coherence, precision, and professional tone of the proposal across key sections.\\nProvide detailed ratings, constructive feedback, and actionable recommendations for improvement.\\n\\n### **Assessment Criteria**\\n\\nEvaluate the proposal based on these aspects of writing style:\\n\\n1. **Clarity**:\\n   - Are technical concepts and methodologies explained clearly and concisely?\\n   - Is the language accessible to an interdisciplinary audience while maintaining technical rigor?\\n   - Are acronyms and specialized terms consistently defined and explained?\\n\\n2. **Precision**:\\n   - Does the writing avoid vague or ambiguous statements?\\n   - Are objectives, methodologies, and outcomes described with sufficient detail and specificity?\\n   - Are claims supported with appropriate data, evidence, or references?\\n\\n3. **Coherence and Flow**:\\n   - Does the document follow a logical sequence of ideas?\\n   - Are transitions between sections and subsections smooth and intuitive?\\n   - Are repetitive or disjointed elements minimized?\\n\\n4. **Professional Tone**:\\n   - Is the writing formal and consistent with academic and scientific standards?\\n   - Does the tone reflect the ambition and significance of the research without overstating claims?\\n   - Are any informal phrases or colloquialisms avoided?\\n\\n5. **Engagement and Persuasiveness**:\\n   - Does the proposal capture the reader’s attention with compelling arguments?\\n   - Are the societal, scientific, and economic impacts of the research effectively communicated?\\n   - Are unique aspects of the research emphasized to distinguish it from similar initiatives?\\n\\n6. **Grammar, Syntax, and Formatting**:\\n   - Are sentences well-constructed, grammatically correct, and free of typographical errors?\\n   - Does the formatting enhance readability, with consistent headings, bullet points, and lists?\\n   - Are visuals, tables, and charts (if applicable) integrated effectively into the narrative?\\n\\n### **Output Requirements**\\n\\n1. Assign a **numerical rating (1–10)** for each criterion.\\n2. Provide **detailed explanations** for each rating, including:\\n   - Specific examples of strong and weak writing practices.\\n   - Suggestions for improving clarity, tone, or engagement.\\n3. Summarize ratings and key findings in a **LaTeX table**.\\n4. Write the report as a **LaTeX document** formatted as follows:\\n   - Use `\\\\section` and `\\\\subsection` for structure.\\n   - Summarize ratings and observations in a formatted table using `\\\\tabular`.\\n   - Include bullet points (`itemize`) to list strengths, weaknesses, and recommendations for each criterion.', 'writing': ' \\n\\n1. Clarity:\\n   - Rating: 7\\n   - The technical concepts and methodologies are explained decently well, but the language could be more accessible to a wider audience. There are some instances of jargon and acronyms that may not be immediately understandable to those outside of the field. For example, the use of \"KB\" for \"knowledge base\" may not be immediately clear to all readers.\\n   - Recommendations: Define and explain technical terms and acronyms more consistently throughout the proposal, especially when they are first introduced. This will help make the language more accessible to a wider audience.\\n\\n2. Precision:\\n   - Rating: 8\\n   - The writing avoids vague or ambiguous statements for the most part. Objectives, methodologies, and outcomes are described with sufficient detail and specificity. However, there are a few instances where claims could be supported with more evidence or references.\\n   - Recommendations: Provide more specific data, evidence, or references to support claims throughout the proposal. This will increase the precision and credibility of the research.\\n\\n3. Coherence and Flow:\\n   - Rating: 6\\n   - The document follows a logical sequence of ideas for the most part, but there are some instances where the flow could be improved. Transitions'}\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "73d14490da09979d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T13:24:21.808668Z",
     "start_time": "2025-01-22T13:24:21.796475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.base import Chain\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "class CustomSequentialChain(Chain, BaseModel):\n",
    "    llm: OpenAI = Field(...)\n",
    "    steps: List[Dict[str, Any]] = Field(...)\n",
    "    output_dir: str = Field(...)\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Input keys required for the first step and any static inputs.\n",
    "        \"\"\"\n",
    "        all_input_keys = set()\n",
    "        for step in self.steps:\n",
    "            all_input_keys.update(step[\"input_keys\"])\n",
    "        return list(all_input_keys)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Output keys of all steps.\n",
    "        \"\"\"\n",
    "        return [step[\"output_key\"] for step in self.steps]\n",
    "\n",
    "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Executes each step sequentially.\n",
    "        \"\"\"\n",
    "        step_inputs = inputs.copy()\n",
    "        step_outputs = {}\n",
    "\n",
    "        for i, step in enumerate(self.steps):\n",
    "            prompt = step[\"prompt\"]\n",
    "            input_keys = step[\"input_keys\"]\n",
    "            output_key = step[\"output_key\"]\n",
    "\n",
    "            # Collect inputs for this step\n",
    "            formatted_inputs = {key: step_inputs[key] for key in input_keys}\n",
    "            formatted_prompt = prompt.format(**formatted_inputs)\n",
    "\n",
    "            # Convert the formatted prompt into a HumanMessage\n",
    "            messages = [HumanMessage(content=formatted_prompt)]\n",
    "\n",
    "            # Run the ChatOpenAI instance with messages\n",
    "            result = self.llm(messages).content\n",
    "            step_outputs[output_key] = result\n",
    "\n",
    "            # Add this step's output to the inputs for the next step\n",
    "            step_inputs[output_key] = result\n",
    "\n",
    "        return step_outputs"
   ],
   "id": "c98f97ea6af315ab",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T13:24:22.516359Z",
     "start_time": "2025-01-22T13:24:22.513850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steps = [\n",
    "    {\n",
    "        'prompt' : writing_style_prompt,\n",
    "        \"input_keys\" : [\"document\",\"prompt1\"],\n",
    "        \"output_key\" : \"writing\",\n",
    "    },\n",
    "    {\n",
    "        'prompt' : research_quality_prompt,\n",
    "        \"input_keys\" : [\"document\",\"prompt2\"],\n",
    "        \"output_key\" : \"quality\",\n",
    "    },\n",
    "    {\n",
    "        'prompt' : flaw_report_prompt,\n",
    "        \"input_keys\" : [\"document\",\"prompt3\"],\n",
    "        \"output_key\" : \"flaws\",\n",
    "    },\n",
    "    {\n",
    "        'prompt' : suggestions_prompt,\n",
    "        \"input_keys\" : [\"document\",\"prompt4\"],\n",
    "        \"output_key\" : \"suggestions\",\n",
    "    }\n",
    "]\n",
    "output_dir = \"output_files\"\n",
    "input_data = {\n",
    "    \"document\" : content_chunks[0],\n",
    "    \"prompt1\" : prompt1_txt,\n",
    "    \"prompt2\" : prompt2_txt,\n",
    "    \"prompt3\" : prompt3_txt,\n",
    "    \"prompt4\" : prompt4_txt,\n",
    "}"
   ],
   "id": "29cd9f944e337fe",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T13:24:23.044301Z",
     "start_time": "2025-01-22T13:24:23.026334Z"
    }
   },
   "cell_type": "code",
   "source": "custom_chain = CustomSequentialChain(llm=llm, steps=steps, output_dir=output_dir)",
   "id": "c7e3766bb39b1127",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatOpenAI' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[98], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m custom_chain \u001B[38;5;241m=\u001B[39m \u001B[43mCustomSequentialChain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_core/load/serializable.py:125\u001B[0m, in \u001B[0;36mSerializable.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\"\"\"\u001B[39;00m\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_openai/llms/base.py:163\u001B[0m, in \u001B[0;36mBaseOpenAI.build_extra\u001B[0;34m(cls, values)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Build extra kwargs from additional params that were passed in.\"\"\"\u001B[39;00m\n\u001B[1;32m    162\u001B[0m all_required_field_names \u001B[38;5;241m=\u001B[39m get_pydantic_field_names(\u001B[38;5;28mcls\u001B[39m)\n\u001B[0;32m--> 163\u001B[0m values \u001B[38;5;241m=\u001B[39m \u001B[43m_build_model_kwargs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_required_field_names\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m values\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_core/utils/utils.py:236\u001B[0m, in \u001B[0;36m_build_model_kwargs\u001B[0;34m(values, all_required_field_names)\u001B[0m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_build_model_kwargs\u001B[39m(\n\u001B[1;32m    220\u001B[0m     values: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m    221\u001B[0m     all_required_field_names: \u001B[38;5;28mset\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m    222\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m    223\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Build \"model_kwargs\" param from Pydanitc constructor values.\u001B[39;00m\n\u001B[1;32m    224\u001B[0m \n\u001B[1;32m    225\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m        ValueError: If a field is specified in model_kwargs.\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 236\u001B[0m     extra_kwargs \u001B[38;5;241m=\u001B[39m \u001B[43mvalues\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m, {})\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m field_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(values):\n\u001B[1;32m    238\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m field_name \u001B[38;5;129;01min\u001B[39;00m extra_kwargs:\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/pydantic/main.py:891\u001B[0m, in \u001B[0;36mBaseModel.__getattr__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m    888\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(item)  \u001B[38;5;66;03m# Raises AttributeError if appropriate\u001B[39;00m\n\u001B[1;32m    889\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    890\u001B[0m     \u001B[38;5;66;03m# this is the current error\u001B[39;00m\n\u001B[0;32m--> 891\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'ChatOpenAI' object has no attribute 'get'"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T13:21:44.461384Z",
     "start_time": "2025-01-22T13:21:43.784776Z"
    }
   },
   "cell_type": "code",
   "source": "outputs = custom_chain(input_data)",
   "id": "9bedee02c7b8d7b",
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotFoundError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[91], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mcustom_chain\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    181\u001B[0m     emit_warning()\n\u001B[0;32m--> 182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain/chains/base.py:389\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    357\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    358\u001B[0m \n\u001B[1;32m    359\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    382\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    384\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    387\u001B[0m }\n\u001B[0;32m--> 389\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain/chains/base.py:170\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    169\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 170\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    171\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    173\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain/chains/base.py:162\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    159\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    160\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m--> 162\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m     )\n\u001B[1;32m    165\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    166\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    167\u001B[0m     )\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "Cell \u001B[0;32mIn[88], line 60\u001B[0m, in \u001B[0;36mCustomSequentialChain._call\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     57\u001B[0m formatted_prompt \u001B[38;5;241m=\u001B[39m prompt\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mformatted_inputs)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# Run the LLM with the formatted prompt\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatted_prompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m step_outputs[output_key] \u001B[38;5;241m=\u001B[39m result\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Write the output to a text file\u001B[39;00m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:182\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    181\u001B[0m     emit_warning()\n\u001B[0;32m--> 182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1277\u001B[0m, in \u001B[0;36m__call__\u001B[0;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m   1259\u001B[0m \u001B[38;5;129m@deprecated\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.1.7\u001B[39m\u001B[38;5;124m\"\u001B[39m, alternative\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minvoke\u001B[39m\u001B[38;5;124m\"\u001B[39m, removal\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1260\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\n\u001B[1;32m   1261\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1268\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m   1269\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m   1270\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Check Cache and run the LLM on the given prompt and input.\u001B[39;00m\n\u001B[1;32m   1271\u001B[0m \n\u001B[1;32m   1272\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   1273\u001B[0m \u001B[38;5;124;03m        prompt: The prompt to generate from.\u001B[39;00m\n\u001B[1;32m   1274\u001B[0m \u001B[38;5;124;03m        stop: Stop words to use when generating. Model output is cut off at the\u001B[39;00m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;124;03m            first occurrence of any of these substrings.\u001B[39;00m\n\u001B[1;32m   1276\u001B[0m \u001B[38;5;124;03m        callbacks: Callbacks to pass through. Used for executing additional\u001B[39;00m\n\u001B[0;32m-> 1277\u001B[0m \u001B[38;5;124;03m            functionality, such as logging or streaming, throughout generation.\u001B[39;00m\n\u001B[1;32m   1278\u001B[0m \u001B[38;5;124;03m        tags: List of tags to associate with the prompt.\u001B[39;00m\n\u001B[1;32m   1279\u001B[0m \u001B[38;5;124;03m        metadata: Metadata to associate with the prompt.\u001B[39;00m\n\u001B[1;32m   1280\u001B[0m \u001B[38;5;124;03m        **kwargs: Arbitrary additional keyword arguments. These are usually passed\u001B[39;00m\n\u001B[1;32m   1281\u001B[0m \u001B[38;5;124;03m            to the model provider API call.\u001B[39;00m\n\u001B[1;32m   1282\u001B[0m \n\u001B[1;32m   1283\u001B[0m \u001B[38;5;124;03m    Returns:\u001B[39;00m\n\u001B[1;32m   1284\u001B[0m \u001B[38;5;124;03m        The generated text.\u001B[39;00m\n\u001B[1;32m   1285\u001B[0m \n\u001B[1;32m   1286\u001B[0m \u001B[38;5;124;03m    Raises:\u001B[39;00m\n\u001B[1;32m   1287\u001B[0m \u001B[38;5;124;03m        ValueError: If the prompt is not a string.\u001B[39;00m\n\u001B[1;32m   1288\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   1289\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(prompt, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m   1290\u001B[0m         msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1291\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1292\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(prompt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. If you want to run the LLM on multiple prompts, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1293\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`generate` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1294\u001B[0m         )\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/llms.py:950\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    944\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\n\u001B[1;32m    945\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    946\u001B[0m )\n\u001B[1;32m    947\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    948\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    949\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[0;32m--> 950\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[1;32m    951\u001B[0m             [prompt],\n\u001B[1;32m    952\u001B[0m             invocation_params\u001B[38;5;241m=\u001B[39mparams,\n\u001B[1;32m    953\u001B[0m             options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m    954\u001B[0m             name\u001B[38;5;241m=\u001B[39mrun_name,\n\u001B[1;32m    955\u001B[0m             batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(prompts),\n\u001B[1;32m    956\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mrun_id_,\n\u001B[1;32m    957\u001B[0m         )[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    958\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m callback_manager, prompt, run_name, run_id_ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\n\u001B[1;32m    959\u001B[0m             callback_managers, prompts, run_name_list, run_ids_list\n\u001B[1;32m    960\u001B[0m         )\n\u001B[1;32m    961\u001B[0m     ]\n\u001B[1;32m    962\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_helper(\n\u001B[1;32m    963\u001B[0m         prompts, stop, run_managers, \u001B[38;5;28mbool\u001B[39m(new_arg_supported), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    964\u001B[0m     )\n\u001B[1;32m    965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/llms.py:792\u001B[0m, in \u001B[0;36m_generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    773\u001B[0m def _generate_helper(\n\u001B[1;32m    774\u001B[0m     self,\n\u001B[1;32m    775\u001B[0m     prompts: list[str],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    779\u001B[0m     **kwargs: Any,\n\u001B[1;32m    780\u001B[0m ) -> LLMResult:\n\u001B[1;32m    781\u001B[0m     try:\n\u001B[1;32m    782\u001B[0m         output = (\n\u001B[1;32m    783\u001B[0m             self._generate(\n\u001B[1;32m    784\u001B[0m                 prompts,\n\u001B[1;32m    785\u001B[0m                 stop=stop,\n\u001B[1;32m    786\u001B[0m                 # TODO: support multiple run managers\n\u001B[1;32m    787\u001B[0m                 run_manager=run_managers[0] if run_managers else None,\n\u001B[1;32m    788\u001B[0m                 **kwargs,\n\u001B[1;32m    789\u001B[0m             )\n\u001B[1;32m    790\u001B[0m             if new_arg_supported\n\u001B[1;32m    791\u001B[0m             else self._generate(prompts, stop=stop)\n\u001B[0;32m--> 792\u001B[0m         )\n\u001B[1;32m    793\u001B[0m     except BaseException as e:\n\u001B[1;32m    794\u001B[0m         for run_manager in run_managers:\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/llms.py:779\u001B[0m, in \u001B[0;36m_generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    768\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m    769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magenerate(\n\u001B[1;32m    770\u001B[0m         prompt_strings, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    771\u001B[0m     )\n\u001B[1;32m    773\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    774\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    775\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m    776\u001B[0m     stop: Optional[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]],\n\u001B[1;32m    777\u001B[0m     run_managers: \u001B[38;5;28mlist\u001B[39m[CallbackManagerForLLMRun],\n\u001B[1;32m    778\u001B[0m     new_arg_supported: \u001B[38;5;28mbool\u001B[39m,\n\u001B[0;32m--> 779\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    780\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    781\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    782\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    783\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[1;32m    784\u001B[0m                 prompts,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    791\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    792\u001B[0m         )\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/langchain_openai/llms/base.py:341\u001B[0m, in \u001B[0;36mBaseOpenAI._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    325\u001B[0m     choices\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m    326\u001B[0m         {\n\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: generation\u001B[38;5;241m.\u001B[39mtext,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    338\u001B[0m         }\n\u001B[1;32m    339\u001B[0m     )\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 341\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_prompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    343\u001B[0m         \u001B[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         \u001B[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mmodel_dump()\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/openai/_utils/_utils.py:279\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 279\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/openai/resources/completions.py:539\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    510\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    537\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    538\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Completion \u001B[38;5;241m|\u001B[39m Stream[Completion]:\n\u001B[0;32m--> 539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    541\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    542\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    543\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    544\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbest_of\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mecho\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mecho\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    548\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msuffix\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    558\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mCompletion\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    570\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/openai/_base_client.py:1283\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1270\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1271\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1278\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1279\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1280\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1281\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1282\u001B[0m     )\n\u001B[0;32m-> 1283\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/openai/_base_client.py:960\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    958\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 960\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/envs/langchain/lib/python3.10/site-packages/openai/_base_client.py:1064\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1061\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1063\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1064\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1066\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[1;32m   1067\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1068\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1072\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[1;32m   1073\u001B[0m )\n",
      "\u001B[0;31mNotFoundError\u001B[0m: Error code: 404 - {'error': {'message': 'This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?', 'type': 'invalid_request_error', 'param': 'model', 'code': None}}"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ad398554d203fc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
